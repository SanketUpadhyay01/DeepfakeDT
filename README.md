AVT2DWF: Audio-Visual Dual Transformers with Dynamic Weight Fusion
Abstract
With the continuous improvements in deepfake methods, forgery messages have transitioned from single-modality to multi-modal fusion, posing new challenges for existing forgery detection algorithms. In this project, we propose AVT2DWF, the Audio-Visual dual Transformers grounded in Dynamic Weight Fusion, which aims to amplify both intra- and cross-modal forgery cues, thereby enhancing detection capabilities. AVT2DWF adopts a dual-stage approach to capture both spatial characteristics and temporal dynamics of facial expressions. This is achieved through a face transformer with an n-framewise tokenization strategy encoder and an audio transformer encoder. Subsequently, it uses multi-modal conversion with dynamic weight fusion to address the challenge of heterogeneous information fusion between audio and visual modalities.

Features
Dual-Stage Approach: Captures both spatial characteristics and temporal dynamics of facial expressions.
Face Transformer: Utilizes an n-framewise tokenization strategy encoder to process visual data.
Audio Transformer: Processes audio data to extract relevant features for forgery detection.
Dynamic Weight Fusion: Addresses the challenge of heterogeneous information fusion between audio and visual modalities.
Enhanced Detection: Amplifies both intra- and cross-modal forgery cues for improved detection accuracy.
